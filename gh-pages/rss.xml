<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title><![CDATA[Primary Unit mirror on GitHub]]></title>
        <link>https://robjwells.github.io</link>
        <atom:link href="https://robjwells.github.io/rss.xml" rel="self" type="application/rss+xml" />
        <description><![CDATA[A mirror of https://www.robjwells.com hosted on GitHub]]></description>
        <lastBuildDate>Sun, 10 Jun 2018 21:59:26 +0000</lastBuildDate>
        <docs>http://blogs.law.harvard.edu/tech/rss</docs>
        <generator>majestic (https://github.com/robjwells/majestic)</generator>
        <item>
            <title><![CDATA[Hard(ware) choices]]></title>
            <link>https://robjwells.github.io/2018/06/hardware-choices/</link>
            <description><![CDATA[<p>It looks like my 2011 iMac might be on the way out. I’ve been having odd graphical problems today and yesterday, and I think that it might be the graphics card overheating. Running Apple Diagnostics (Hardware Test as was) reports an error with the hard drive (<a href="https://www.ifixit.com/Answers/View/488225/Apple+Hardware+Test+Result+%3A+4M0T-4-40000003%3A+HDD-1510">the fan specifically</a>) which I’ve seen before. My working theory at this point is that a failed or obstructed fan coupled with dust build-up and the fairly hot room has led to this point.</p>
<p class="full-width">
<img alt="Photo of an iMac where part of the right-hand section of the screen image is displayed physically on the left" src="https://robjwells.github.io/images/2018-06-10-imac-graphics-problem.jpeg"/>
</p>
<p>I’m going to get some compressed air and see if that helps matters at all, and then see if I can get it serviced. Unfortunately Apple now lists the 2011-model iMac as obsolete (or “vintage”) so we’ll see how that goes.</p>
<p>Funnily enough, this is not the original graphics card but a replacement installed by Apple when something similar (but not quite the same) happened several years ago, sorted out just before their replacement period ended (it was an acknowledged, somewhat widespread problem with the cards).</p>
<p>Honestly it’s happened at a bit of a naff time. The machine is otherwise fine, and it still feels incredibly fast (which I put down to the SSD). I was hoping it would last as long as many of the machines we have at work, almost all of which are from 2008 (despite what other press reports claim), though they do feel sluggish — particularly the couple that I (foolishly) upgraded past OS X 10.6. I was certainly not planning to replace it yet.</p>
<p>It would be an expense that I could do without too — having just bought <a href="https://www.back2.co.uk/okamura-cp-mesh-chair-black-silver-frame.html">a real chair</a> and <a href="https://www.kinesis-ergo.com/shop/advantage2/">weird keyboard</a> in anticipation of having to do much more work at home from October when I start an evening Masters in Computer Science at <a href="http://www.birkbeck.ac.uk/">Birkbeck</a>.</p>
<p>Which brings us neatly to the real annoyance about this: I was planning to buy a laptop to use on the course. I’ve bought a couple of bottom-end MacBook Airs for reporters at work, and they seem like decent enough machines. This idea was on the assumption that it would not be my main computer, so it could be less capable as I would be using it for focused tasks and leaving everything else — including much of the academic work of the course — to be done on my giant iMac at home.</p>
<p>But if the iMac isn’t in the picture anymore, what should I do? As I see it, I have two feasible options:</p>
<ul>
<li>Buy a more capable laptop as my main computer.</li>
<li>Buy a basic laptop and a new iMac.</li>
</ul>
<p>I spent a lot of money on the 27" iMac in October 2011, buying more than I needed really (partly because I was still playing computer games then). I wouldn’t replace it with something as high-end now as I know that I just don’t need the power, and I want to minimise the hit to my savings. </p>
<p>Part of me thinks that I should buy a MacBook Air now, according to plan but sooner than planned. Then I have something to tide me over other than a six-year-old iPad (running iOS 9!) and a five-year-old iPhone 5S, and then I can try to get the iMac fixed or replace it at a later date. (I’m also a bit anxious to buy the Air sooner rather than later, even though it’s still basically an old design, as I don’t really want to risk having to buy a laptop with a <a href="https://theoutline.com/post/2402/the-new-macbook-keyboard-is-ruining-my-life">dodgy keyboard</a> and only a couple of those odd USB-C sockets.)</p>
<p>Then the other option is to shelve completely the idea of buying another desktop and just buy a more powerful laptop. This appeals to me less, because I do want a bigger screen and I do want more storage.</p>
<p>I’ve been throwing the storage matter around in my head today and I can’t decide on a position. Backblaze tells me I have 530GB backed up, most of which is my iTunes library, so I don’t know if it’s a bit of a distraction — if I only had a laptop I’d have to keep it on an external drive at home, and is that so different from keeping it only in an iMac on my desk?</p>
<p>It’s difficult to know what to do. My gut feeling is to rush out and buy a basic Air immediately so as to not interrupt my life too much (particularly, I need to change jobs before the course starts as the hours don’t fit; this is not a secret to my boss or colleagues).</p>
<p>But my head says that I should see if I can crack on using my iPad for now, enquire about getting the iMac serviced as soon as possible, and then make a more considered decision at a later time.</p>
<p>It’s not a comfortable position for me, since this machine has been a fixture in my life for over six-and-a-half years. When the graphics card went last time, it was calming to know that it was a problem that Apple had committed to take care of (even if I almost missed the cutoff and had to drag the heavy thing all the way to Chafford Hundred on the train). No such luck this time.</p>
<p>(On a, er, positive note, I guess this will finally force me to work out a way of blogging from iPad that isn’t as painful as I’m sure it will be to get this post up.)</p>]]></description>
            <pubDate>Sun, 10 Jun 2018 21:50:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/06/hardware-choices/</guid>
        </item>
        <item>
            <title><![CDATA[Jupyter notebook macro bug]]></title>
            <link>https://robjwells.github.io/2018/05/jupyter-notebook-macro-bug/</link>
            <description><![CDATA[<p>My post detailing <a href="https://robjwells.github.io/2018/05/open-jupyter-notebooks-with-a-keyboard-maestro-macro/">a Keyboard Maestro macro to open Jupyter notebooks</a> had a dumb bug in the second shell pipeline, which fetches the URL of the desired notebook.</p>
<p>You’d hit it if:</p>
<ul>
<li>You have more than one notebook server running.</li>
<li>The working directory of one is beneath another.</li>
<li>The subdirectory server was started more recently.</li>
<li>You tried to open the parent server with the macro.</li>
</ul>
<p>The shorter path of the parent would match part of the child’s path.</p>
<p>The original <code>grep</code> pattern was:</p>
<pre><code>grep "$KMVAR_dir"</code></pre>
<p>And is now:</p>
<pre><code>grep ":: $KMVAR_dir$"</code></pre>
<p>So that it only matches the exact directory chosen in the list prompt, and not one of its children.</p>
<p>I’ve updated <a href="https://robjwells.github.io/files/OpenJupyterNotebook.kmmacros">the Keyboard Maestro macro file</a> too.</p>]]></description>
            <pubDate>Tue, 22 May 2018 09:47:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/jupyter-notebook-macro-bug/</guid>
        </item>
        <item>
            <title><![CDATA[HTML image dimensions with a CSS border]]></title>
            <link>https://robjwells.github.io/2018/05/html-image-dimensions-with-a-css-border/</link>
            <description><![CDATA[<p>When I use images here, I tend to give ones without any transparency a border, which is done using CSS and applied to <code>img</code> tags unless they have a <code>no-border</code> class.</p>
<p>Like a good web citizen, I also <a href="http://w3c.github.io/html/semantics-embedded-content.html#example-82f39213">specify image dimensions in HTML</a>:</p>
<blockquote>
<p>“The image’s rendered size is given in the width and height attributes, which allows the user agent to allocate space for the image before it is downloaded.”</p>
</blockquote>
<p>In fact my BBEdit <code>image</code> snippet makes it a doddle:</p>
<pre><code>&lt;p &lt;#* class="full-width"#&gt;&gt;
    &lt;img
        src="/images/#SELECTIONORINSERTION#"
        alt="&lt;#alt text#&gt;"
        &lt;#* class="no-border"#&gt;
        width=&lt;#width#&gt;
        height=&lt;#height#&gt;
        /&gt;
&lt;/p&gt;</code></pre>
<p>But this causes a problem, which I’ve spotted in a couple of my recent posts.</p>
<p>If you specify the image dimensions, and use a CSS border, <em>and</em> have your CSS <code>box-sizing</code> set to <code>border-box</code>, then the CSS border shrinks the amount of space available to the image to its specified dimensions − 2 × the border width.</p>
<p>So if you specify your <code>img</code> dimensions to match the dimensions of the file, then the image itself will be shrunk within the element.</p>
<p>This animation shows this situation, and what happens when you toggle the CSS border. Watch what happens to the image itself.</p>
<p class="full-width">
<img alt="An animation showing an image being squeezed within the space it has been allocated, causing distortion." class="no-border" height="385" src="https://robjwells.github.io/images/2018-05-17-border-with-dimensions.gif" width="565"/>
</p>
<p>(It’s got a slight offset from the text because it’s a screenshot of this blog and includes some of the background on each side.)</p>
<p>In contrast, this animation shows what happens when the dimensions are not specified, and so the image is free to grow when the border is applied:</p>
<p class="full-width">
<img alt="An animation showing an image growing when a CSS border is applied, with no distortion to the image itself." class="no-border" height="385" src="https://robjwells.github.io/images/2018-05-17-border-no-dimensions.gif" width="565"/>
</p>
<p>Really the culprit here is <code>box-sizing: border-box</code>, forcing the border to remain within the size of the <code>img</code> element itself. This is a behaviour you actually want, as it solves the old CSS problem of juggling widths, borders and padding within a parent element. Check out <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/box-sizing">MDN’s <code>box-sizing</code> page</a> to see what I mean.</p>
<p>What are my options, then?</p>
<ul>
<li>
<p>Change box-sizing.</p>
<p>I’m not touching this because the potential sizing headaches are not worth it, even just for <code>img</code> elements.</p>
</li>
<li>
<p>Apply a border to the image files themselves.</p>
<p>No, because if I change my mind about the CSS, previously posted images are stuck with the old style forever. CSS borders should also work correctly across high-density displays, whereas a 1px border in the file may not.</p>
</li>
<li>
<p>Don’t specify dimensions in the HTML.</p>
<p>I don’t like the idea of making pages of this site slower to render, but I think this is the least bad option, particularly given that <a href="https://robjwells.github.io/2017/04/page-speed/">this site is already pretty fast</a>.</p>
</li>
</ul>
<p>It’s not ideal, but that BBEdit snippet is now just:</p>
<pre><code>&lt;p &lt;#* class="full-width"#&gt;&gt;
    &lt;img
        src="/images/#SELECTIONORINSERTION#"
        alt="&lt;#alt text#&gt;"
        &lt;#* class="no-border"#&gt;
        /&gt;
&lt;/p&gt;</code></pre>
<p>Hey, at least it makes images quicker to include in posts!</p>]]></description>
            <pubDate>Thu, 17 May 2018 21:00:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/html-image-dimensions-with-a-css-border/</guid>
        </item>
        <item>
            <title><![CDATA[Open Jupyter notebooks with a Keyboard Maestro macro]]></title>
            <link>https://robjwells.github.io/2018/05/open-jupyter-notebooks-with-a-keyboard-maestro-macro/</link>
            <description><![CDATA[<p>I have a startup item that launches a Jupyter notebook so that the server is always running in the background. It’s an attempt to reduce the friction of using the notebooks.</p>
<p>By default, Jupyter starts the server on port 8888 on localhost, but expects a token (a long hexadecimal string) before it’ll let you in. If you list the currently running servers in the terminal you can see the token and also the server’s working directory.</p>
<pre><code>% jupyter-notebook list
Currently running servers:
http://localhost:8889/?token=…hex… :: /Users/robjwells
http://localhost:8888/?token=…hex… :: /Users/robjwells/jupyter-notebooks</code></pre>
<p>We can use this to make finding and opening the particular notebook server you want a bit easier, using Keyboard Maestro.</p>
<p>
<img alt="A screenshot showing the (minimised) Keyboard Maestro steps" src="https://robjwells.github.io/images/2018-05-17-macro-overview.png"/>
</p>
<p>The macro uses the <code>jupyter-notebook</code> command, so that’ll need to be in your <code>$PATH</code> as Keyboard Maestro sees it.</p>
<p>The first and third steps both execute <code>jupyter-notebook list</code> and use Unix tools to extract parts from it.</p>
<p>In between, if there’s more than one notebook server running, the macro prompts the user to choose one from a list of their working directories.</p>
<p>
<img alt="A Keyboard Maestro list selection dialogue" class="no-border" height="464" src="https://robjwells.github.io/images/2018-05-17-notebook-list.png" width="534"/>
</p>
<p>Here’s the first step, where we fetch the list of working directories.</p>
<pre><code>jupyter-notebook list | tail -n +2 | awk '{print $3}'</code></pre>
<p>Our +2 argument to <code>tail</code> gets the output from the second line, chopping off the “Currently running servers:” bit. Then <code>awk</code> prints the third field, which contains the directory. (The first is the URL, the second the double-colon separator.)</p>
<p>The third step fetches the corresponding URL for a directory:</p>
<pre><code>jupyter-notebook list | grep ":: $KMVAR_dir$" | awk '{ print $1 }'</code></pre>
<p>Since the user has specified a directory already, we use <code>grep</code> with the Keyboard Maestro variable to find just that one line, and use <code>awk</code> again to extract the URL field.</p>
<div class="flag" id="update-20180522">
<p><strong>Update: <time>2018-05-22</time></strong></p>
<p>There was a bug in the original version of this snippet of shell script, where a parent path could match a child path (as it was only looking for the path itself without an anchor on either side). It was only luck that had me miss this with my example, with the more recently started home directory notebook server being listed ahead of one in a subdirectory, which <code>grep</code> would have also matched. The code above and the macro file have been fixed.</p>
</div>
<p>Obviously, this won’t work if you have more than one notebook server running from the same directory. (But you wouldn’t do that, right?)</p>
<p><a href="https://robjwells.github.io/files/OpenJupyterNotebook.kmmacros">Here’s the macro file</a> if you’d like to try it out.</p>]]></description>
            <pubDate>Thu, 17 May 2018 19:45:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/open-jupyter-notebooks-with-a-keyboard-maestro-macro/</guid>
        </item>
        <item>
            <title><![CDATA[Horizontally scroll a table in HTML]]></title>
            <link>https://robjwells.github.io/2018/05/horizontally-scroll-a-table-in-html/</link>
            <description><![CDATA[<p>After I published <a href="https://www.robjwells.com/2018/05/table-manipulation-with-r/">my post about manipulating tables (of data) in R</a>, I noticed that there was something amiss with the HTML table in that post showing an example section of our newsroom rota.</p>
<p class="full-width">
<img alt="A screenshot showing a table laid out with table-layout: fixed in CSS, with many cells wrapping with scrollbars in an unreadable fashion." src="https://robjwells.github.io/images/2018-05-11-table-fixed.png"/>
</p>
<p>When I first wrote the CSS for this site, <a href="https://www.robjwells.com/2013/07/five-different-kinds-of-grey/">roughly five years ago</a>, I had HTML tables set so that the whole table would scroll were it to be too wide for its containing column. At least, I’m pretty sure it worked like that.</p>
<p>Anyway, as you can see above, it doesn’t work like that now. The table there is laid out with the following CSS:</p>
<pre><code>table {
  table-layout: fixed;
  width: 100%;
}</code></pre>
<p>Which has the effect of restricting the table size to 100%, and doing odd things to the cells if there’s too much to fit in whatever width 100% happens to be.</p>
<p>As an attempted quick fix, I removed the <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/table-layout"><code>table-layout</code></a> property so that it would inherit the default, <code>auto</code>. The width is still 100% to provide some consistency, rather than having an odd assortment of table widths.</p>
<p>So the CSS is now this:</p>
<pre><code>table {
  width: 100%;
}</code></pre>
<p>This has the effect of having the table overflow the container horizontally if the content is too wide, like so:</p>
<p class="full-width">
<img alt="A screenshot showing a table laid out with table-layout: auto in CSS, with the table overflowing its container horizontally." src="https://robjwells.github.io/images/2018-05-11-table-auto.png"/>
</p>
<p>Which is perhaps more readable if pretty ugly. And not what I wanted: to scroll the entire table within its container.</p>
<p>I said attempted earlier because I, er, never deployed the change on the site (it’s been a busy couple of weeks, contrary to the post tempo).</p>
<p>In the meantime, I stumbled across a fix by opening Safari’s reader mode, in which tables scroll horizontally within their container! The secret? The table is wrapped in an enclosing <code>div</code>, which has its <code>overflow-x</code> property set to <code>auto</code>, and then the table scrolls within the div.</p>
<p>Here’s what that looks like when rendered:</p>
<p class="full-width">
<img alt="A screenshot showing a table laid out and scrolling within a containing div with its overflow-x property set to auto." src="https://robjwells.github.io/images/2018-05-11-table-div.png"/>
</p>
<p>Here’s the HTML:</p>
<pre><code>&lt;div class="table-container"&gt;
  &lt;table&gt;
  …
  &lt;/table&gt;
&lt;/div&gt;</code></pre>
<p>And here’s the CSS:</p>
<pre><code>table {
  width: 100%;
}

.table-container {
  overflow-x: auto;
}</code></pre>
<p>You want <code>auto</code> instead of <code>scroll</code> as the latter shows the scrollbar all the time. </p>]]></description>
            <pubDate>Fri, 11 May 2018 23:35:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/horizontally-scroll-a-table-in-html/</guid>
        </item>
        <item>
            <title><![CDATA[Table manipulation with R]]></title>
            <link>https://robjwells.github.io/2018/05/table-manipulation-with-r/</link>
            <description><![CDATA[<p>One of my responsibilities at work is to provide a list of people who our printers should call if there’s ever a problem with the edition. Usually that’s the chief sub, or whoever is covering her.</p>
<p>I also prepare the rota for the journalistic staff, which I use as the source of information for the responsibility list.</p>
<p>This job has largely escaped automation. I do have a Python script that prints a nice template report for the week ahead, complete with BBEdit placeholders, but working out whose name should be attached to each edition is just done by reading the rota across and deleting names from the template list until you’re down to one.</p>
<p>However, I’ve found things of this nature, if not automated, are put off, forgotten, or done wrong. This, because it’s not actually vital to anything, is no exception, particularly when I’m pulled into jobs that actually are vital.</p>
<p>The report looks a little like this, so you get the idea:</p>
<pre><code>Tue May 08    16pp    Alice Jones
Wed May 09    16pp    Bob Smith
Thu May 10    16pp    Rob Wells</code></pre>
<p>And so on, with the pagination in the middle column.</p>
<p>The pagination is consistent (16 in the week, 24 on the weekend) with occasional larger editions. It can either be predicted with total certainty or none at all, as the large editions vary considerably with advertising and feature articles.</p>
<p>The responsibility can’t be predicted because we don’t work fixed patterns (we don’t have enough staff to do so). However, it can be done in advance once the newsroom rota is completed.</p>
<p>So let’s forget the pagination and just focus on pulling together a list of every production day in the completed period and who is the chief sub.</p>
<p>Our newsroom rota is just a spreadsheet, which is actually the best tool I’ve found so far for handling a couple dozen people with intricate job-cover links between them. (The rota used to be laid out in InDesign, which, no matter what you think about spreadsheets or InDesign, was much more difficult.)</p>
<p>It looks a bit like this (the real spreadsheet has proper formatting and so on):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Sun 6/5</th>
<th>Mon 7/5</th>
<th>Tue 8/5</th>
<th>Wed 9/5</th>
<th>Thu 10/5</th>
<th>Fri 11/5</th>
<th>Sat 12/5</th>
<th>Lieu add</th>
<th>Lieu tot</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rob Wells</td>
<td>Off</td>
<td></td>
<td></td>
<td>Sport</td>
<td>Ch Sub</td>
<td>Sport</td>
<td></td>
<td></td>
<td>10</td>
</tr>
<tr>
<td>Alice Jones</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Off</td>
<td></td>
<td></td>
<td>0.25</td>
<td>4.5</td>
</tr>
</tbody>
</table>
</div>
<p>There’s a fair amount of information: names, dates, days off, cover responsibilities, new and accrued <a href="https://www.gov.uk/overtime-your-rights/time-off-and-paid-leave">TOIL</a>. It’s entirely designed for humans, not computers (and it takes the humans a little while until they’re able to read it).</p>
<p>A lot is implicit. If we assume in this example that Alice is the chief sub, she is performing that role on her usual working days (the empty cells). It is only marked for people who have to cover someone else’s job.</p>
<p>This table is not something that you can just chuck into a computer program; it needs cleaning up first.</p>
<p>Thankfully, R (and the <a href="https://www.tidyverse.org">Tidyverse</a> particularly) is a great environment in which to wrangle your data, and to do so fairly quickly. All the code below was pulled together in about 30 minutes total (with a good 10 minutes of reading documentation and fixing errors in the original source data). Writing this post has taken much longer.</p>
<p>In our example below we’re going to have four workers who each cover the chief sub at different times. Here we’re going make “Dan Taylor” the chief sub. Congratulations, Dan!</p>
<p>First we’ll pull in our libraries.</p>
<pre><code>library(tidyverse)
library(lubridate)
library(stringr)</code></pre>
<p>Then we’ll read in the data, which is saved in a TSV file after copying and pasting from the spreadsheet into a text document. We’ll select only the production days and the unnamed first column (named X1 on import), excluding Saturdays and the TOIL columns.</p>
<pre><code>wide &lt;- read_tsv('chsub.tsv') %&gt;%
    select(matches('^(Mon|Tue|Wed|Thu|Fri|Sun) |X1')) %&gt;%
    rename(name = X1)</code></pre>
<p>Then we’ll use a <a href="http://tidyr.tidyverse.org">tidyr</a> function, <a href="http://tidyr.tidyverse.org/reference/gather.html"><code>gather()</code></a>, to transform our wide format into a tall one by selecting the date columns. It’s easier to get a feel for <code>gather()</code> by looking at the output.</p>
<pre><code>tidy &lt;- wide %&gt;%
    gather(matches('^(Mon|Tue|Wed|Thu|Fri|Sun) '),
           key = date,
           value = status)
head(tidy)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 3
##   name           date     status
##   &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;
## 1 Alice Jones    Sun 29/4 Off
## 2 Bob Smith      Sun 29/4 Sick
## 3 Carol Williams Sun 29/4 Booked
## 4 Dan Taylor     Sun 29/4 &lt;NA&gt;
## 5 Alice Jones    Mon 30/4 &lt;NA&gt;
## 6 Bob Smith      Mon 30/4 Off</code></pre>
<p>We now have a row for each person for each day, along with their “status” for the day.</p>
<p>But Dan doesn’t have his chief sub days marked, as it would be nearly every day. Let’s split out Dan’s rows and replace the empty cells with <code>Ch Sub</code>, the same status string used by everyone else. Then we’ll combine the filled-out Dan rows with all the non-Dan rows from the original data frame.</p>
<pre><code>dan_replaced &lt;- tidy %&gt;%
    filter(name == 'Dan Taylor') %&gt;%
    replace_na(list(status = 'Ch Sub'))

all &lt;- tidy %&gt;%
    filter(name != 'Dan Taylor') %&gt;%
    rbind(dan_replaced)

tail(all)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 3
##   name       date      status
##   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;
## 1 Dan Taylor Sun 30/12 Ch Sub
## 2 Dan Taylor Mon 31/12 Ch Sub
## 3 Dan Taylor Tue 1/1   Ch Sub
## 4 Dan Taylor Wed 2/1   Ch Sub
## 5 Dan Taylor Thu 3/1   Ch Sub
## 6 Dan Taylor Fri 4/1   Ch Sub</code></pre>
<p>Great. But poor Dan, he’s working every day over New Year 2018-2019. In reality, I haven’t done that far on the rota, just up to October. We’ll convert all those dates now, and filter out all the newly missing entries where the month was outside our range.</p>
<pre><code>dated &lt;- all %&gt;%
    mutate(
        date = dmy(str_c(str_extract(date, '\\d+/[4-9]'), '/2018'))
    ) %&gt;%
    filter(!is.na(date))</code></pre>
<p>Let’s get only the chief sub-related rows and sort them by date.</p>
<pre><code>chsub &lt;- dated %&gt;%
    filter(str_detect(status, 'Ch Sub')) %&gt;%
    arrange(date) %&gt;%
    select(date, chief_sub = name)
head(chsub)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 2
##   date       chief_sub
##   &lt;date&gt;     &lt;chr&gt;
## 1 2018-04-29 Dan Taylor
## 2 2018-04-30 Dan Taylor
## 3 2018-05-01 Dan Taylor
## 4 2018-05-02 Bob Smith
## 5 2018-05-03 Dan Taylor
## 6 2018-05-04 Carol Williams</code></pre>
<p>Exactly what we want. Now time for a bit of formatting to make this giant list somewhat acceptable for other people. This is also where my knowledge of R runs out.</p>
<pre><code>formatted &lt;- str_c(
    format(chsub$date, '%a %Y-%m-%d'),
           chsub$chief_sub,
           sep = '  ')

fd &lt;- file('output.txt')
writeLines(formatted, fd)
close(fd)</code></pre>
<p>So we’ll switch to Python, printing a blank line between each production week (of six days).</p>
<pre><code>with open('output.txt') as f:
    for idx, line in enumerate(f.readlines()):
        if idx % 6 == 0:
            print()
        print(line, end='')</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>Sun 2018-08-19  Carol Williams
Mon 2018-08-20  Carol Williams
Tue 2018-08-21  Alice Jones
Wed 2018-08-22  Carol Williams
Thu 2018-08-23  Carol Williams
Fri 2018-08-24  Carol Williams

Sun 2018-08-26  Carol Williams
Mon 2018-08-27  Carol Williams
Tue 2018-08-28  Carol Williams
Wed 2018-08-29  Dan Taylor
Thu 2018-08-30  Dan Taylor
Fri 2018-08-31  Dan Taylor</code></pre>
<p>Perfect. And ready for whenever I get time to update the rota again.</p>]]></description>
            <pubDate>Tue, 08 May 2018 23:15:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/table-manipulation-with-r/</guid>
        </item>
        <item>
            <title><![CDATA[Working with R Markdown]]></title>
            <link>https://robjwells.github.io/2018/05/working-with-r-markdown/</link>
            <description><![CDATA[<p>I’ve just published an update to <a href="https://www.robjwells.com/2018/05/3-75-years-on-the-tube/">my recent Tube travel post</a>, fixing a few small mistakes, a bigger one (an error in a station name that nonetheless didn’t affect the plot involved) and adding an update to the last section which goes a bit deeper into the fare and duration difference between the two periods.</p>
<p>I didn’t fix the mistake in the title, as I felt it was too late, but of course it’s 3⅔ years, not 3.75, since September 2014.</p>
<p>
<video controls="" muted="" poster="https://robjwells.github.io/images/2018-05-05-naked-gun-realise-that-now.jpg" src="https://robjwells.github.io/images/2018-05-05-naked-gun-realise-that-now.mp4">
<img alt="“I realise that, now…” from the film Naked Gun" src="https://robjwells.github.io/images/2018-05-05-naked-gun-realise-that-now.jpg"/>
</video>
</p>
<p>In that post I mentioned how pleasant it is working in <a href="https://www.rstudio.com/products/RStudio/">R Studio</a> in a <a href="https://rmarkdown.rstudio.com">R Markdown</a> document. It really is, and I find the R Markdown way of mixing prose and code much more natural and fluid than <a href="https://jupyter.org">Jupyter notebooks</a>, which I like the idea of but find the block-based method a bit awkward.</p>
<p>The biggest problem with R Markdown was fitting it into my, admittedly arcane, <a href="https://github.com/robjwells/majestic">blogging system</a>. To do so, I’ve cooked up <a href="https://github.com/robjwells/primaryunit/blob/3be7c91007f10946e60fecb3c2007f85080d3950/posts/2018/04/decode_blocks.py">a short Python script</a> to transform the Markdown output from R Studio and <a href="https://yihui.name/knitr/">knitr</a>.</p>
<p>Right now, I’ve settled on this set of output options in the YAML front-matter:</p>
<pre><code>md_document:
    variant: markdown_strict+fenced_code_blocks
    preserve_yaml: true
    fig_width: 7.5
    fig_height: 5
    dev: svg
    pandoc_args: [
        "--wrap", "preserve"
    ]</code></pre>
<p>Now I don’t actually use fenced (<code>~~~~</code>) code blocks in Markdown, instead I just use regular Markdown indented code blocks with a header line (<code>python:</code>) at the top. But I include that extension in the Markdown variant to make it easier to transform code blocks later.</p>
<p>But why? Well, if your output just uses indented code blocks, it’s difficult to tell which of those are your R code and which are the R code’s output. Fencing the blocks makes it easier to insert empty comments after each block, keeping code and output separate.</p>
<p>The <a href="https://en.wikipedia.org/wiki/YAML">YAML</a> front matter is preserved as I use a similar thing in my own posts and this gets passed through to my blogging system without a problem, with unknown settings ignored. (I do remove the quoting that the template file includes around strings.)</p>
<p>The other important option above is supplying the <code>--wrap</code> argument to <a href="http://pandoc.org">Pandoc</a>, preserving the line breaks as they are in the source file instead of breaking them. By default Pandoc hard-wraps the lines, which I’d be fine with, except that it hard wraps the alt text for images (plots).</p>
<p>That makes it more difficult to pick out later. This is necessary as I always use HTML to include images in my posts (so I can set classes, allow for full-width etc).</p>
<p>I say more difficult as I’m working line-wise. It’d be possible to apply a regex to the joined lines and make the transformation, but then again I don’t hard-wrap my own posts so it’s not something I care about keeping.</p>
<p>The option I would like to use is to keep my Markdown reference links intact, instead of having Pandoc put everything inline. But this makes the images into reference links, making rewriting more difficult again.</p>
<p>So, I knit the document together from R Studio, then apply <a href="https://github.com/robjwells/primaryunit/blob/3be7c91007f10946e60fecb3c2007f85080d3950/posts/2018/04/decode_blocks.py">the script</a>, and pipe the output into the for-real .md file. This is the one that gets checked into the <a href="https://www.mercurial-scm.org">Mercurial</a> repository, fed into my blog generator and ultimately published.</p>
<p>I could probably get away with doing less, or handling things differently — such as allowing for fenced code on the generator’s side.</p>
<p>But I want the transformed output to resemble as closely as possible something that I’d written in <a href="https://www.barebones.com/products/bbedit/">BBEdit</a> because I actually attach some importance to the contents of the Markdown files outside their use as raw material with which to create HTML.</p>
<p>They should be able to tell the post’s story without needing to be processed further, to interpret R code or make the raw source readable. I’m not quite at the point of having totally pure, completely readable plain text files (note those dummy comments mentioned above) but I want to be as close as I can.</p>]]></description>
            <pubDate>Sat, 05 May 2018 23:55:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/working-with-r-markdown/</guid>
        </item>
        <item>
            <title><![CDATA[3.75 years on the Tube]]></title>
            <link>https://robjwells.github.io/2018/05/3-75-years-on-the-tube/</link>
            <description><![CDATA[<p>A couple of years ago, shortly after I moved house, I wrote <a href="https://robjwells.github.io/2016/09/two-years-on-the-tube/">a post analysing my Tube travel</a>. It was my first real attempt to do that kind of analysis, and the first time I’d done anything with <a href="https://matplotlib.org">Matplotlib</a> of any level of complexity.</p>
<p>I thought I’d have another crack at it now, looking at the changes in my travel patterns since the move, and also changing from Python and Matplotlib to R and ggplot2.</p>
<p>Why now? There’s no great immediate reason, though for a time I was thinking about stopping to use my Oyster card in favour of a contactless bank card. You don’t have the option to be emailed CSV journey history files with a bank card, and the main advantage of weekly capping wouldn’t affect me, so I’ll be sticking with the Oyster card for the moment.</p>
<p>But, as I noted in the introduction to the previous post, my travel habits have changed considerably. Before I would commute by train twice a day, whereas now I’m within a short cycle of work. I’m expecting this to have a significant effect in what we observe below.</p>
<p>And why the switch in environment? Python is still the language that fits my brain the best, but Matplotlib feels like hard work. R is a pretty odd language in many ways, but the ggplot2 way of building plots makes a great deal of sense to me, and has allowed me to play with plots quickly in ways that I feel that wouldn’t be available if I was trying to contort to fit Matplotlib’s preferences. I freely admit that I don’t have a great deal of experience with Matplotlib, so it’s entirely possible that’s the reason why I find it a struggle, but that barrier just isn’t there with ggplot2.</p>
<p>I’m writing this post in <a href="https://www.rstudio.com/products/RStudio/">RStudio</a> in a <a href="https://rmarkdown.rstudio.com">R Markdown</a> document, but it’s actually my second go at this. The first was invaluable in getting myself acquainted with the process and playing around with ideas, but it kind of spiralled out of control so it’s not presentable. Hopefully this is something approaching readable.</p>
<h3>Setup</h3>
<p>To start with we’re going to load some libraries to make our life easier. The <a href="https://www.tidyverse.org">Tidyverse</a> wraps up several helpful packages; lubridate has some handy date-handling functions; stringr is helpful for, er, strings; patchwork allows you to easily combine plots into one figure; ggalt provides an extra geom (<code>geom_encircle()</code>) that we’ll use in a bit. Forgive me for not making clear where functions come from below as, like Swift, R imports into the global namespace.</p>
<p>Not shown is my customised ggplot2 theme, which you can find if you <a href="https://github.com/robjwells/primaryunit/tree/master/posts/2018/04">look at the original .Rmd source file</a>.</p>
<pre><code>library(tidyverse)
library(lubridate)
library(stringr)
library(patchwork)
library(ggalt)

# Moving average function from https://stackoverflow.com/a/4862334/1845155
mav &lt;- function(x, n) {
    stats::filter(x, rep(1/n, n), sides = 1)
}</code></pre>
<!-- Comment to separate R code and output -->
<h3>Data import</h3>
<p>I keep all the CSV files as received, just dating the filenames with the date I got them. (Sorry, I won’t be sharing the data.) Let’s load all the files:</p>
<pre><code>oyster_filenames &lt;- dir(
    '~/Documents/Oyster card/Journey history CSVs/',
    pattern = '*.csv',
    full.names = TRUE)</code></pre>
<!-- Comment to separate R code and output -->
<p>There are 109 CSV files that we need to open, load, and combine.</p>
<pre><code>oyster_data &lt;- oyster_filenames %&gt;%
    map(~ read_csv(., skip = 1)) %&gt;%
    reduce(rbind)</code></pre>
<!-- Comment to separate R code and output -->
<p>Here we’re piping <code>oyster_filenames</code> through <code>map</code>, where we use an R formula to supply arguments to <code>read_csv</code> to skip the header line in each file. Finally we <code>reduce</code> the 109 data frames by binding them by row.</p>
<h3>Poking around the data</h3>
<p>We can take a look at the data to get an idea of its structure:</p>
<pre><code>head(oyster_data)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 8
##   Date   `Start Time` `End Time` `Journey/Action`    Charge Credit Balance
##   &lt;chr&gt;  &lt;time&gt;       &lt;time&gt;     &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;
## 1 31-Oc… 23:22        23:50      North Greenwich to…    1.5 &lt;NA&gt;     26.0 
## 2 31-Oc… 18:39        18:59      Woolwich Arsenal D…    1.6 &lt;NA&gt;     27.6 
## 3 31-Oc… 18:39           NA      Auto top-up, Woolw…   NA   20       29.2 
## 4 31-Oc… 17:10        17:37      Stratford to Woolw…    1.6 &lt;NA&gt;      9.15
## 5 31-Oc… 16:26        16:53      Woolwich Arsenal D…    1.6 &lt;NA&gt;     10.8 
## 6 30-Oc… 22:03        22:39      Pudding Mill Lane …    1.5 &lt;NA&gt;     12.4 
## # ... with 1 more variable: Note &lt;chr&gt;</code></pre>
<p>It’s clearly in need of a clean-up. The journey history file appears to be a record of every action involving the card. It’s interesting to note that the Oyster card isn’t just a “key” to pass through the ticket barriers, but a core part of how the account is managed (note that having an online account is entirely optional).</p>
<p>Actions taken “outside” of the card need to be “picked up” by the card by tapping on an Oyster card reader. Here we can see a balance increase being collected, mixed in with the journey details. (Funnily enough, TfL accidentally cancelled my automatic top-up a couple of months ago, but that was never applied to my account as I didn’t use the card before the action expired.)</p>
<p>But we’re only interested in rail journeys, one station to another, with a start and finish time.</p>
<p>Let’s see if the notes field can give us any guidance of what we may need to exclude.</p>
<pre><code>oyster_data %&gt;%
    filter(!is.na(Note)) %&gt;%
    count(Note, sort = TRUE)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 5 x 2
##   Note                                                                   n
##   &lt;chr&gt;                                                              &lt;int&gt;
## 1 The fare for this journey was capped as you reached the daily cha…    18
## 2 We are not able to show where you touched out during this journey      6
## 3 This incomplete journey has been updated to show the &lt;station&gt; yo…     1
## 4 We are not able to show where you touched in during this journey       1
## 5 You have not been charged for this journey as it is viewed as a c…     1</code></pre>
<p>OK, not much here, but there are some troublesome rail journeys missing either a starting or finishing station. The “incomplete journey” line also hints at something to be aware of:</p>
<pre><code>oyster_data %&gt;%
    filter(str_detect(Note, 'This incomplete journey')) %&gt;%
    select(`Journey/Action`) %&gt;%
    first()</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## [1] "Woolwich Arsenal DLR to &lt;Blackheath [National Rail]&gt;"</code></pre>
<p>Note the angle brackets surrounding the substituted station. We’ll come back to this later.</p>
<p>A missing start or finish time is a giveaway for oddities, which overlaps somewhat but not completely with Journey/Action fields that don’t match the pattern of <code>{station} to {station}</code>. Let’s fish those out and have a look at the abbreviated descriptions:</p>
<pre><code>stations_regex &lt;- '^&lt;?([^&gt;]+)&gt;? to &lt;?([^&gt;]+)&gt;?$'

oyster_data %&gt;%
    filter(
        is.na(`Start Time`) |
        is.na(`End Time`) |
        !str_detect(`Journey/Action`, stations_regex)) %&gt;%
    mutate(abbr = str_extract(`Journey/Action`, '^[^,]+')) %&gt;%
    count(abbr, sort = TRUE)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 11 x 2
##    abbr                                              n
##    &lt;chr&gt;                                         &lt;int&gt;
##  1 Auto top-up                                      84
##  2 Bus journey                                      26
##  3 Automated Refund                                  7
##  4 Woolwich Arsenal DLR to [No touch-out]            3
##  5 Oyster helpline refund                            2
##  6 Unknown transaction                               2
##  7 [No touch-in] to Woolwich Arsenal DLR             1
##  8 Entered and exited Woolwich Arsenal DLR           1
##  9 Monument to [No touch-out]                        1
## 10 Stratford International DLR to [No touch-out]     1
## 11 Stratford to [No touch-out]                       1</code></pre>
<h3>Tidying the data</h3>
<p>All these should be filtered out of the data for analysis. (The two unknown transactions appear to be two halves of my old commute. Strange.)</p>
<pre><code>rail_journeys &lt;- oyster_data %&gt;%
    # Note the !() below to invert the earlier filter
    filter(!(
        is.na(`Start Time`) |
        is.na(`End Time`) |
        !str_detect(`Journey/Action`, stations_regex)))</code></pre>
<!-- Comment to separate R code and output -->
<p>That leaves us with 993 rail journeys to have a look at.</p>
<p>But there’s more tidying-up to do:</p>
<ul>
<li>Journey dates and times are stored separately. Finish times may be after midnight (and so on the day after the date they’re associated with).</li>
<li>Start and finish stations need to be separated. (And don’t forget that set of angle brackets.)</li>
<li>All money-related fields should be dropped except for “charge” (the journey fare).</li>
</ul>
<p>Let’s have a crack at it, proceeding in that order:</p>
<pre><code>tidy_journeys &lt;- rail_journeys %&gt;%
    mutate(
        start = dmy_hms(
            str_c(Date, `Start Time`, sep=' '),
            tz = 'Europe/London'),
        end = dmy_hms(
            str_c(Date, `End Time`, sep=' '),
            tz = 'Europe/London') +
            # Add an extra day if the journey ends “earlier” than the start
            days(1 * (`End Time` &lt; `Start Time`)),
        # Let’s add a duration to make our lives easier
        duration = end - start,

        enter = str_match(`Journey/Action`, stations_regex)[,2],
        exit = str_match(`Journey/Action`, stations_regex)[,3]
    ) %&gt;%
    select(
        start, end, duration,
        enter, exit,
        fare = Charge
    ) %&gt;%
    # Sorting solely to correct the slightly odd example output
    arrange(start)
head(tidy_journeys)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 6
##   start               end                 duration enter    exit      fare
##   &lt;dttm&gt;              &lt;dttm&gt;              &lt;time&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;
## 1 2014-09-06 13:14:00 2014-09-06 13:42:00 28       Woolwic… Stratfo…   1.5
## 2 2014-09-06 13:59:00 2014-09-06 14:08:00 9        Stratfo… Hackney…   1.5
## 3 2014-09-06 20:47:00 2014-09-06 21:02:00 15       Hackney… Highbur…   1.5
## 4 2014-09-06 23:22:00 2014-09-07 00:10:00 48       Highbur… Woolwic…   2.7
## 5 2014-09-07 10:00:00 2014-09-07 10:30:00 30       Woolwic… Pudding…   1.5
## 6 2014-09-07 20:43:00 2014-09-07 21:15:00 32       Pudding… Woolwic…   1.5</code></pre>
<p>Great. The duration variable isn’t strictly necessary but it’ll make things a tad clearer later on.</p>
<h3>Weekly totals</h3>
<p>For a start, let’s try to remake the first plot from <a href="https://robjwells.github.io/2016/09/two-years-on-the-tube/">my previous post</a>, of weekly spending with a moving average.</p>
<p>Looking back, it’s not tremendously helpful, but it’s a starting point. (In addition, while that plot is labelled as showing a six-week average, the code computes <a href="https://github.com/robjwells/primaryunit/blob/master/posts/2016/09/analyse_journey_history.py#L168">an eight-week average</a>, and a quick count of the points preceding the average line confirms it.)</p>
<p>But there’s a problem with the data: they record journeys made, not the absence of any journeys (obviously). If we’re to accurately plot weekly spending, we need to include weeks where no journeys were made and no money spent.</p>
<p>First, let’s make a data frame containing every <a href="https://en.wikipedia.org/wiki/ISO_week_date">ISO week</a> from the earliest journey in our data to the most recent.</p>
<pre><code>blank_weeks &lt;- seq(min(tidy_journeys$start),
    max(tidy_journeys$end),
    by = '1 week') %&gt;%
    tibble(
        start = .,
        week = format(., '%G-W%V')
    )
head(blank_weeks)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 2
##   start               week    
##   &lt;dttm&gt;              &lt;chr&gt;   
## 1 2014-09-06 13:14:00 2014-W36
## 2 2014-09-13 13:14:00 2014-W37
## 3 2014-09-20 13:14:00 2014-W38
## 4 2014-09-27 13:14:00 2014-W39
## 5 2014-10-04 13:14:00 2014-W40
## 6 2014-10-11 13:14:00 2014-W41</code></pre>
<p>The format string uses the ISO week year (%G) and the ISO week number (%V), which may differ from what you might intuitively expect. I’ve included a somewhat arbitrary start time, as it’s a bit easier to plot and label datetimes rather than the year-week strings.</p>
<p>Now we need to summarise our actual journey data, collecting the total fare for each ISO week. We’ll use <code>group_by()</code> and <code>summarise()</code> — two tools that took me a few tries to get a handle on. Here <code>summarise()</code> works group-wise based on the result of <code>group_by()</code>; you don’t have to pass the group into the <code>summarise()</code> call, just specify the value you want summarised and how.</p>
<pre><code>real_week_totals &lt;- tidy_journeys %&gt;%
    group_by(week = format(start, '%G-W%V')) %&gt;%
    summarise(total = sum(fare))</code></pre>
<!-- Comment to separate R code and output -->
<p>That done, we can use an SQL-like join operation to take every week in our giant list and match it against the week summaries from our real data. The join leaves missing values (<code>NA</code>) in the total column for weeks where no journeys were made (and so weren’t present in the data to summarise) so we replace them with zero.</p>
<pre><code>complete_week_totals &lt;- left_join(blank_weeks,
                                  real_week_totals,
                                  by = 'week') %&gt;%
    replace_na(list(total = 0))
tail(complete_week_totals)</code></pre>
<!-- Comment to separate R code and output -->
<pre><code>## # A tibble: 6 x 3
##   start               week     total
##   &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt;
## 1 2018-03-17 12:14:00 2018-W11   0  
## 2 2018-03-24 12:14:00 2018-W12   0  
## 3 2018-03-31 13:14:00 2018-W13  21.1
## 4 2018-04-07 13:14:00 2018-W14   9.5
## 5 2018-04-14 13:14:00 2018-W15   0  
## 6 2018-04-21 13:14:00 2018-W16   7.8</code></pre>
<p>With this summary frame assembled, we can now plot the totals. I’m also going to mark roughly when I moved house so we can try to see if there’s any particular shift.</p>
<pre><code>house_move &lt;- as.POSIXct('2016-08-01')
pound_scale &lt;- scales::dollar_format(prefix = '£')

weeks_for_avg &lt;- 8

ggplot(data = complete_week_totals,
       mapping = aes(x = start, y = total)) +
    geom_vline(
        xintercept = house_move,
        colour = rjw_grey,
        alpha = 0.75) +
    geom_point(
        colour = rjw_blue,
        size = 0.75) +
    geom_line(
        mapping = aes(y = mav(complete_week_totals$total,
                              weeks_for_avg)),
        colour = rjw_red) +

    labs(
        title = str_glue(
            'Weekly transport spending and {weeks_for_avg}',
            '-week moving average'),
        subtitle = (
            'September 2014 to May 2018, vertical bar marks house move'),
        x = NULL, y = NULL) +

    scale_x_datetime(
        date_breaks = '6 months',
        date_labels = '%b ’%y') +
    scale_y_continuous(
        labels = pound_scale)</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A plot showing my weekly Oyster card spending, September 2014 to May 2018" class="no-border" src="https://robjwells.github.io/images/2018-05-03-weekly-spending-1.svg" width="720"/>
</p>
<p>It’s clear that there is a difference after the house move. But I’m not sure this plot is the best way to show it. (Nor the best way to show anything.)</p>
<p>That said, the code for this plot is a pretty great example of what I like about ggplot2: you create a plot, add geoms to it, customise the labels and scales, piece by piece until you’re happy. It’s fairly straightforward to discover things (especially with RStudio’s completion), and you change things by adding on top of the basics instead of hunting around in the properties of figures or axes or whatever.</p>
<h3>Cumulative spending</h3>
<p>The first plot showed a change in my average weekly spending. What does that look like when we plot the cumulative spending over this period?</p>
<pre><code>ggplot(data = tidy_journeys,
       mapping = aes(x = start,
                     y = cumsum(fare),
                     colour = start &gt; house_move)) +
    geom_line(
        size = 1) +

    labs(
        title = 'Cumulative Oyster card spending',
        subtitle = 'September 2014 to May 2018',
        x = NULL, y = NULL,
        colour = 'House move') +
    scale_y_continuous(
        labels = pound_scale,
        breaks = c(0, 500, 1000, 1400, 1650)) +
    scale_color_brewer(
        labels = c('Before', 'After'),
        palette = 'Set2') +
    theme(
        legend.position = 'bottom')</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A plot showing my cumulative Oyster card spending, September 2014 to May 2018" class="no-border" src="https://robjwells.github.io/images/2018-05-03-cumulative-spending-1.svg" width="720"/>
</p>
<p>The difference in slope is quite clear; at one point I fitted a linear smoother to the two periods but it overlapped so tightly with the data that it was difficult to read either. I’ve also monkeyed around with the y-axis breaks to highlight the difference; what before took three to six months to spend has taken about 21 months since the house move.</p>
<h3>Zero-spending weeks</h3>
<p>One thing that shows up in the first plot, and likely underlies the drop in average spending, is the number of weeks where I don’t travel using my Oyster card at all. Let’s pull together a one-dimensional plot showing just that.</p>
<pre><code>ggplot(complete_week_totals,
       aes(x = start,
           y = 1,
           fill = total == 0)) +
    geom_col(
        width = 60 * 60 * 24 * 7) +  # datetime col width handled as seconds
    geom_vline(
        xintercept = house_move,
        colour = rjw_red) +

    scale_fill_manual(
        values = c(str_c(rjw_grey, '20'), rjw_grey),
        labels = c('Some', 'None')) +
    scale_x_datetime(
        limits = c(min(complete_week_totals$start),
                   max(complete_week_totals$start)),
        expand = c(0, 0)) +
    scale_y_continuous(
        breaks = NULL) +
    labs(
        title = 'Weeks with zero Oyster card spending',
        subtitle = 'September 2014 to May 2018, red line marks house move',
        x = NULL, y = NULL,
        fill = 'Spending') +
    theme(
        legend.position = 'bottom')</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A plot showing weeks where I made no journeys using my Oyster card" class="no-border" src="https://robjwells.github.io/images/2018-05-03-zero-spending-weeks-1.svg" width="720"/>
</p>
<p>The change here after I moved house is stark, nearly an inversion of the previous pattern of zero/no-zero spending weeks. (Almost looks like <a href="https://www.robjwells.com/2018/02/british-newspaper-barcodes-explained-and-automated/">a barcode</a>!)</p>
<p>My apologies for the thin lines between columns, which is an SVG artefact. The inspiration for this was a plot of games/non-games in the App Store top charts that <a href="http://leancrew.com/all-this/">Dr Drang</a> included at the bottom of one of his posts and, for the life of me, I can’t find now.</p>
<h3>Changes in journey properties</h3>
<p>So it’s clear that I travel less on the Tube network, and that I spend less. But what has happened to the sort of journeys that I make? Are they longer? Shorter? Less expensive? More?</p>
<p>Let’s have a look at how the average fare and average journey duration change over time.</p>
<pre><code>n_journey_avg &lt;- 10

common_vline &lt;- geom_vline(xintercept = house_move,
                           colour = rjw_red)
common_point &lt;- geom_point(size = .5)

fares_over_time &lt;- ggplot(tidy_journeys,
                          aes(x = start,
                              y = mav(fare, n_journey_avg))) +
    scale_x_datetime(
        labels = NULL) +
    scale_y_continuous(
        labels = pound_scale) +
    labs(
        y = 'Fare',
        title = 'More expensive, shorter journeys',
        subtitle = str_glue('{n_journey_avg}-journey average, ',
                            'vertical line marks house move'))

duration_over_time &lt;- ggplot(tidy_journeys,
                             aes(x = start,
                                 y = mav(duration, n_journey_avg))) +
    scale_y_continuous() +
    labs(
        y = 'Duration (mins)')

(fares_over_time / duration_over_time) &amp;  # Patchwork is magic
    common_vline &amp;
    common_point &amp;
    labs(x = NULL)</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A plot of average fares and journey durations over time" class="no-border" src="https://robjwells.github.io/images/2018-05-03-fare-duration-averages-1.svg" width="720"/>
</p>
<p>Journeys taken after the house move appear to be shorter and more expensive. How distinct is this? What is driving the averages? I have a hunch so let me rush on ahead with this plot.</p>
<pre><code>commute_stations &lt;- c('Woolwich Arsenal DLR', 'Stratford International DLR',
                      'Stratford', 'Pudding Mill Lane DLR')

commute_journeys &lt;- tidy_journeys %&gt;%
    filter(
        enter %in% commute_stations,
        exit %in% commute_stations)

high_speed_journeys &lt;- tidy_journeys %&gt;%
    filter(
        str_detect(enter, 'HS1'),
        str_detect(exit, 'HS1'))

ggplot(tidy_journeys,
       aes(x = fare,
           y = duration,
           colour = start &gt; house_move)) +
    geom_jitter(
        width = 0.05,  # 5p
        height = 0.5,  # 30 seconds
        alpha = 0.5) +
    geom_encircle(
        data = commute_journeys,
        size = 1.5) +
    geom_encircle(
        data = high_speed_journeys,
        size = 1.5) +

    scale_color_brewer(
        palette = 'Set2',
        labels = c('Before', 'After')) +
    scale_x_continuous(
        labels = pound_scale) +
    scale_y_continuous(
        limits = c(0, 80)) +
    labs(
        title = 'Pre- and post-move averages driven by two groups',
        subtitle = str_c('Old commute and high-speed journeys circled,',
                         ' positions not exact'),
        x = 'Fare',
        y = 'Duration (mins)',
        colour = 'House move')</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A plot of journey fare against distance, grouped by whether the journeys were before or after I moved house" class="no-border" src="https://robjwells.github.io/images/2018-05-03-fare-duration-scatter-1.svg" width="720"/>
</p>
<p>We can see in the lower central section that there’s some overlap. Remember also that there are far fewer post-move journeys, so it’s not surprising that earlier ones dominate this plot. (I added jitter to the points to make things a little easier to see — <code>geom_jitter()</code> is a wrapper around <code>geom_point()</code>.)</p>
<p>But what is crucial to understanding the averages are the two rough groups circled: journeys between stations that I used for my old commute (on the left in green), and journeys involving travel on the <a href="https://en.wikipedia.org/wiki/High_Speed_1">High Speed 1</a> (HS1) rail line (on the right in orange).</p>
<p>My old commute was low-cost, each way either £1.50 or £1 (with an off-peak railcard discount, applied for part of the pre-move period). There are a lot of these journeys (nearly 500). It was a fairly predictable 30ish-minute journey.</p>
<p>On the other hand, trips involving the HS1 line are expensive and very short. A single off-peak fare is currently £3.90 and peak £5.60, while the journey time between Stratford International and St Pancras is just seven minutes, with a bit of waiting inside the gateline.</p>
<h3>But is that it?</h3>
<p>Does that theory of the two extreme groups really explain the difference? Let’s filter out the two groups from our journey data.</p>
<pre><code>journeys_without_extremes &lt;- tidy_journeys %&gt;%
    anti_join(commute_journeys) %&gt;%
    anti_join(high_speed_journeys)</code></pre>
<!-- Comment to separate R code and output -->
<p>Let’s look how the journey durations compare:</p>
<pre><code>ggplot(journeys_without_extremes,
       aes(x = duration,
           fill = start &gt; house_move)) +
    geom_histogram(
        binwidth = 5,
        closed = 'left',
        colour = 'black',
        size = 0.15,
        position = 'identity') +
    scale_x_continuous(
        breaks = seq(0, 70, 10),
        limits = c(0, 70)) +
    scale_fill_brewer(
        palette = 'Set2',
        labels = c('Before', 'After')) +
    labs(
        title = 'Post-move journeys still shorter',
        subtitle = 'Commute and HS1 journeys excluded, bars overlap',
        x = 'Duration (mins)',
        y = 'Number of journeys',
        fill = 'House move')</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A histogram showing journey durations having excluded known extremes, with post-move journeys generally shorter" class="no-border" src="https://robjwells.github.io/images/2018-05-03-duration-hist-without-extremes-1.svg" width="720"/>
</p>
<p>And the fares:</p>
<pre><code>ggplot(journeys_without_extremes,
       aes(x = fare,
           fill = start &gt; house_move)) +
    geom_histogram(
        binwidth = 0.5,
        closed = 'left',
        colour = 'black',
        size = 0.15,
        position = 'identity') +
    scale_x_continuous(
        labels = pound_scale) +
    scale_fill_brewer(
        palette = 'Set2',
        labels = c('Before', 'After')) +

    labs(
        title = 'Post-move journeys generally more expensive',
        subtitle = 'Commute and HS1 journeys excluded, bars overlap',
        x = 'Fare',
        y = 'Number of journeys',
        fill = 'House move')</code></pre>
<!-- Comment to separate R code and output -->
<p class="full-width">
<img alt="A histogram showing journey fares having excluded known extremes, with post-move fares generally more expensive" class="no-border" src="https://robjwells.github.io/images/2018-05-03-fare-hist-without-extremes-1.svg" width="720"/>
</p>
<p>While it’s much clearer for duration than cost now, post-move journeys are still generally shorter and more expensive.</p>
<p>At this point, I’ve reached the limits of how far I’m able to take this with visualisation. One possible route would be to look at the distance between station (in miles), how many stations used are in which fare zone, and the number of fare zones crossed. I don’t have stations/fare zones data readily to hand so we’ll leave that here.</p>
<p>But I’ll end with an intuitive answer. Durations are shorter because from Woolwich it takes additional time to get into the main Tube network from the DLR, and particularly to central stations. Whereas now I’m not far from a Central Line station, which will get me into zone 1 fairly quickly.</p>
<p>Fares are higher because I’ve transferred classes of journeys to cycling — not just my commute to work but shopping and leisure. I’d reckon that the remaining journeys are more likely to involve travel into and within central London, and maybe more likely to be at peak times.</p>
<h3>Last thoughts</h3>
<p>If you made it this far, well done, and thanks for reading. There’s a lot of R code in this post, probably too much. But there are two reasons for that: as a reference for myself, and to show that there’s not any magic going on behind the curtain, and very little hard work. (In my code at least, there’s plenty of both in the libraries!)</p>
<p>Working in R with ggplot2 and the other packages really is a pleasure; it doesn’t take very long to grasp how the different tools fit together into nice, composable pieces, and to assemble them in ways that produce something that matches what you have pictured in your mind.</p>]]></description>
            <pubDate>Thu, 03 May 2018 01:00:00 +0100</pubDate>
            <guid>https://robjwells.github.io/2018/05/3-75-years-on-the-tube/</guid>
        </item>
        <item>
            <title><![CDATA[British newspaper barcodes explained — and automated]]></title>
            <link>https://robjwells.github.io/2018/02/british-newspaper-barcodes-explained-and-automated/</link>
            <description><![CDATA[<p>Barcodes can be pretty mystifying from the outside, if all you’ve got to go on is a set of lines and numbers, or even magic incantations for the software that produces them.</p>
<p>Despite working at a place where we produce a product with a new barcode every day, I didn’t understand how they were made up for years.</p>
<p>But they’re fairly straightforward, and once you know how they work it’s quite simple to produce them reliably. That’s important because getting a barcode wrong can cause real problems.</p>
<h3>Barcode problems</h3>
<p>In the case we’ll look at here, daily newspapers, an incorrect barcode means serious headaches for your wholesalers and retailers, and you’ll likely and entirely understandably face a penalty charge for them having to handle your broken product.</p>
<p>I know because, well, I’ve been there. In our case at the Star there were two main causes of incorrect barcodes, both down to people choosing:</p>
<ol>
<li>the wrong issue number or sequence variant;</li>
<li>the wrong barcode file.</li>
</ol>
<p>We’ll talk about the terminology shortly, but we can see how easily problem number one can occur by looking at the interface of standard barcode-producing software:</p>
<p>
<img alt="A screenshot of the interface of Agamik BarCoder, a good barcode-producing application" class="no-border" height="626" src="https://robjwells.github.io/images/2018-02-28-agamik.png" width="526"/>
</p>
<p>Now, <a href="http://www.agamik.co.uk/index.php">Agamik BarCoder</a> is a nice piece of software and is very versatile. If you need to make a barcode it’s worth a look.</p>
<p>But look again at that interface — it’s not intuitive what you need to do to increment from the previous day’s barcode, the settings for which are saved in the application. It’s very easy to put in the wrong details, or accidentally reuse yesterday’s details.</p>
<p>Second, it produces files with names such as <code>ISSN 03071758_23_09</code> — a completely logical name, but the similarity between the names and the fact you have to manually place the file on your page makes it easy to choose the wrong barcode, whose name will likely differ only by one digit to the previous day.</p>
<p>That isn’t helped by Adobe InDesign by default opening the last-used folder when you place an image. At least once, I’ve made the barcode first thing in the morning and accidentally placed the previous day’s barcode file.</p>
<p>One of the suggestions we had after we printed a paper with the wrong barcode was to have the barcode checked twice before the page is sent to the printers. This is an entirely sensible suggestion, but I know from experience that — however well-intentioned — “check <code>x</code> twice” is a rule that will be broken when you’re under pressure and short-staffed.</p>
<p>It’s far more important to have a reliable production process so that whatever makes it through to the proofreading stage is certain to be correct, or as close as possible.</p>
<p>We can understand this by looking at the <a href="https://en.wikipedia.org/wiki/Hierarchy_of_hazard_controls">hierarchy of hazard controls</a>, which is useful far outside occupational health and safety:</p>
<p>
<img alt="An illustration of the hierarchy of controls, to reduce industry hazards, which has at the top (most effective) the elimination of hazards, followed by substitution, engineering controls, administrative controls and then finally (and least effective) personal protective equipment." class="no-border" height="396" src="https://robjwells.github.io/images/2018-02-28-hierarchy-of-controls.png" width="560"/>
</p>
<p>“Check twice” is clearly an administrative control — changing the way people work while leaving the hazard in place. An engineering control in our case might be to have software check the barcode when the page is about to be sent to the printers (something we do on PDF export by inspecting the filename). We want to aim still higher up the hierarchy, eliminating or substituting the hazard.</p>
<p>But to reach that point we need to understand the components of a barcode.</p>
<h3>Barcode components</h3>
<p>Barcodes are used all over the place, so it’s understandable that some terms are opaque. But picking a specific case — daily newspaper barcodes here — it’s quite easy to break down what they mean and why they’re important.</p>
<p>The information here comes from <a href="http://www.anmw.co.uk/anmw/documents/PPA_Barcode_Guidelines.pdf">the barcoding guidance published by the Professional Publishers Association and Association of Newspaper and Magazine Wholesalers</a>. It’s a very clear document and if you’re involved in using barcodes for newspapers or magazines you should read it. (Really, do read it, as while I’ll try to bring newspaper barcodes “to life” below, there’s a lot of information in there that I won’t cover — such as best practice for sizing.)</p>
<p>Let’s start off by examining a typical newspaper EAN-13+2 barcode, using the terms that you’ll find in the PPA-ANMW guidance:</p>
<p class="full-width">
<img alt="A diagram showing the components of a British newspaper barcode, using the EAN-13+2 format." class="no-border" height="418" src="https://robjwells.github.io/images/2018-02-28-issn-basics.png" width="720"/>
</p>
<p>You’ll see at first that it’s clearly made up of two components: the largest is a typical <a href="https://en.wikipedia.org/wiki/International_Article_Number">EAN-13</a> barcode with a smaller <a href="https://en.wikipedia.org/wiki/EAN-2">EAN-2</a> on the right.</p>
<p>Reading left-to-right, we have the <a href="https://www.gs1.org/company-prefix">GS1 prefix</a> to the barcode number, which is always 977 for the <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number">ISSN numbers</a> assigned to newspapers and magazines.</p>
<p>Next is the first seven digits of the publication’s ISSN number — the eighth digit isn’t included because it is a <a href="https://en.wikipedia.org/wiki/Check_digit">check digit</a> and is redundant because the EAN-13 includes its own check digit.</p>
<p>That check digit follows a two-digit sequence variant, which encodes some information about the periodical. On the right, above the EAN-2, is the issue number. This is used in different ways depending on the publication’s frequency.</p>
<p>Lastly is a chevron, which is used to guard some amount of whitespace on the right-hand side to ensure the barcode reader has enough room to scan properly. (The leading 9 performs the same function on the left.) This is optional.</p>
<h3>In practice</h3>
<p>Now let’s look at a real barcode, see which elements we have to think about, and how they fit together.</p>
<p class="full-width">
<img alt="A diagram showing an annotated barcode as used by the Morning Star newspaper." class="no-border" height="440" src="https://robjwells.github.io/images/2018-02-28-ms-barcode-annotated.png" width="720"/>
</p>
<p>Now let’s start with the elements that were present on the basic ISSN barcode.</p>
<h4>ISSN number</h4>
<p>Your newspaper’s ISSN appears after the 977 prefix. The Morning Star’s ISSN is 0307-1758, but the 8 at the end of that is a check digit, used to detect errors in the preceding seven digits. This is removed because it’s unnecessary as the 13th digit of the EAN-13 is a check digit for all 12 preceding digits. So only the front seven digits of the ISSN appear in the bar code.</p>
<h4>Sequence variant</h4>
<p>For daily newspapers the sequence variant provides two pieces of information.</p>
<p>The first digit is a price code, which indicates to retailers what price they should charge. The code is dependent on the publication — you can’t tell from the price code alone what price a newspaper will be. For the Star, we currently use price codes 2 (£1) and 4 (£1.50).</p>
<p>The second digit is the <a href="https://en.wikipedia.org/wiki/ISO_week_date">ISO weekday number</a>. Monday is ISO weekday 1, through to Sunday as 7.</p>
<p>So by looking at the sequence variant in this barcode, we can tell that it’s the paper for Wednesday (ISO weekday 3) and sells at whatever price code 2 corresponds to in the retailer’s point-of-sale system.</p>
<p>When you introduce a new price, typically you use the next unused price code. We recently increased the price of our Saturday paper from £1.20 (price code 3) to £1.50 (price code 4).</p>
<h4>Issue number</h4>
<p>The issue number appears above the EAN-2 supplemental barcode. For daily newspapers this corresponds to the <a href="https://en.wikipedia.org/wiki/ISO_week_date">ISO week</a> containing the edition. Note that this may differ from, say, the week number in your diary. New ISO weeks begin on Monday.</p>
<p>Modern versions of <code>strftime</code> accept the <code>%V</code> format, which will return a zero-padded ISO week number. In Python the <code>date</code> and <code>datetime</code> classes have an <a href="https://docs.python.org/3/library/datetime.html#datetime.datetime.isocalendar"><code>.isocalendar()</code> method</a> which returns a 3-tuple of ISO week-numbering year, ISO week number and ISO weekday number.</p>
<h4>Header strap</h4>
<p>The line printed above the barcode is technically not part of the barcode itself, and different publications do different things. It’s common not to print anything, and for years we didn’t either, but I think it’s quite useful to print related information here to help whoever has to check the barcode before the page is sent for printing.</p>
<p>Note that in this example, all the information printed above the barcode is referred to in the barcode itself (except the year). I use this space to “decode” the barcode digits for human readers.</p>
<p>This was suggested to me by our printers (Trinity Mirror), who do something similar with their own titles.</p>
<h4>Light margin indicator</h4>
<p>Eagle-eyed readers will spot that the chevron used to guard whitespace for the barcode scanner is missing from the right-hand side. The PPA-ANMW guidance does urge that you include the chevron, but its absence as such won’t cause scanning problems.</p>
<p>It’s straightforward to guarantee enough space around the barcode by carefully placing it in the first place. Our back-page template reserves a space for the barcode, along with some legally required information, which is big enough to make the chevron unnecessary. You can see this in the image below:</p>
<p class="full-width">
<img alt="An annotated photo of the barcode on a printed copy of the Morning Star, noting the space reserved around the barcode." src="https://robjwells.github.io/images/2018-02-28-ms-paper-annotated.jpg"/>
</p>
<p>The main block of text on the left of the barcode doesn’t change. The date below it does, but it’s been tested so that even the longest dates provide enough space. (The longest date in consideration being the edition of Saturday/Sunday December 31-January 1 2022-2023.)</p>
<p>The superimposed purple lines show where the margins would appear in Adobe InDesign, with the barcode in the bottom-right corner. This section is ruled off above to prevent the encroachment of page elements, with the understanding that page items must end on the baseline grid line above the rule (which itself sits on the grid).</p>
<p class="pull-right">
<img alt="A photograph of a older style of Morning Star barcode, showing page elements in close proximity." src="https://robjwells.github.io/images/2018-02-28-morningstar-old-barcode.jpg"/>
<img alt="A photograph of an even older style of Morning Star barcode, showing page elements in close proximity and a light margin chevron." src="https://robjwells.github.io/images/2018-02-28-morningstar-older-barcode.jpg"/>
</p>
<p>(As you can see from the smaller photos, this wasn’t always the case. The barcode often had page elements very close by, and did not have its own clear space. At this point, the barcode was also produced at a smaller size to fit within one of the page’s six columns.)</p>
<p>The “inside margin” on the right-hand side of the page (remember that the back page is in fact the left-hand page of a folded spread) provides an additional light margin. However, note that you still need an adequate distance from the fold itself:</p>
<blockquote>
<p>“it is recommended that the symbol should not be printed closer than 10 mm from any cut or folded edge” (PPA-ANMW)</p>
</blockquote>
<p>Our inside margin is 9mm, with the edge of the EAN-2 symbol roughly 1.5mm further in, for a total 10.5mm. While it appears that there’s bags of space, we’re still only just within the recommendations.</p>
<p>You might want to put the barcode on the outer edge of the back (the left-hand side) as the margin there is deeper (15mm in our case), but I would be very cautious about doing so. I’ve seen enough mishandled papers with bits torn off that I prefer the safety of the inside of the sheet.</p>
<p>You can see similar considerations at work when you look at how other papers place their barcodes. This example of the Sunday Mirror is quite similar to the Morning Star above, but rotated to make use of the more abundant vertical space:</p>
<p class="full-width">
<img alt="A photograph of a barcode on the back of the Sunday Mirror, rotated so that it is placed sideways on the page." src="https://robjwells.github.io/images/2018-02-28-sundaymirror-barcode.jpg"/>
</p>
<p>(You can also see the use of a strap above the barcode, with the title name (SM, Sunday Mirror) and date (210517). I’m not sure what LO means, but it could mean London, if this is used as a way of identifying batches from different print sites.)</p>
<p class="pull-right">
<img alt="A photo of the barcode on The Times newspaper." src="https://robjwells.github.io/images/2018-02-28-thetimes-barcode.jpg"/>
<img alt="A photo of the barcode on the Financial Times newspaper." src="https://robjwells.github.io/images/2018-02-28-financialtimes-barcode.jpg"/>
</p>
<p>The Times and Financial Times also take this approach of cordoning off a space. Neither use a header strap (not unusual), though I am confused by the placement of the chevron in the FT’s barcode. It should be outside of the symbol area to reserve the space, though a lack of space is certainly not an issue.</p>
<p>Dedicating some space for the barcode is important because it means that there won’t be any compromises made day-to-day. You’ll want to take into account the recommended size and magnification factors in the PPA-ANMW guidance if adjusting page templates.</p>
<p>One of the changes we made was to abandon the reduced-size barcode (to fit within a page column), which then meant that something else was needed to fill out two columns to justify the space. But — as seen in the examples from other papers — it might be that having some amount of additional blank space around the barcode is an easy sell anyway.</p>
<h3>Automation</h3>
<p>Where these considerations really come in is when you automate the creation and setting of the barcode, because they can be thought about once, agreed and then left untouched as the system ticks along.</p>
<p>This gets us to the substitution level of the hierarchy of controls — we’re looking to do away with the hazard of human error in barcode creation, but ultimately we replace it with another hazard, ensuring that an automated system works correctly. We’ll return to this hazard briefly after taking a look at the automation program itself.</p>
<p><a href="https://github.com/ppps/ms-barcode">The code is available on GitHub</a>. I won’t be including large chunks of it because it’s all fairly nuts and bolts stuff (and this post is long enough already!).</p>
<p>The structure is fairly straightforward. Like a lot of my more recent automation projects at work, it has an AppleScript user interface which passes arguments to a Python command-line program, which either performs some action itself or returns a value for use in the AppleScript program.</p>
<p>In this case, the Python program computes the correct sequence variant (price and weekday) and issue number (ISO week) — along with a human-readable header — and embeds them in a <a href="https://en.wikipedia.org/wiki/PostScript">PostScript</a> program that uses the brilliant <a href="https://bwipp.terryburton.co.uk">BWIPP</a> barcode library.</p>
<p>This PostScript is processed into a PDF file by <a href="https://en.wikipedia.org/wiki/Ghostscript">Ghostscript</a>, and the path to this barcode PDF is handed back to the AppleScript program so that it can embed it in a labelled frame in InDesign. (To embed files in an InDesign document you’ll need the <code>unlink</code> verb. Yes, I thought that meant “delete the link” at first as well.)</p>
<p>Here’s a diagram to show the flow through the program (forgive the graphics, I’m learning how to use OmniGraffle):</p>
<p class="full-width">
<img alt="A diagram showing the flow of action through the ms-barcode application. An AppleScript UI takes input, Python organises the creation of the barcode (using BWIPP and Ghostscript) and then returns the barcode PDF file path to AppleScript, which then embeds it in an Adobe InDesign file." class="no-border" src="https://robjwells.github.io/images/2018-02-28-ms-barcode-diagram.png"/>
</p>
<p><a href="https://github.com/AlDanial/cloc">Cloc</a> tells me that the main Python file has a <em>whopping</em> 104 lines of code, and there are 264 lines of code in the related unit tests.</p>
<p>Really all of the heavy lifting is done by BWIPP, a cut-down version of which is included in the <a href="https://github.com/ppps/ms-barcode">ms-barcode</a> repository (just ISSN, EAN-13 and EAN-2). The entirety of my “own” PostScript is this (where the parts in braces are Python string formatting targets):</p>
<pre><code>%!PS
({bwipp_location}) run

11 5 moveto ({issn} {seq:02} {week:02}) (includetext height=1.07)
  /issn /uk.co.terryburton.bwipp findresource exec

% Print header line(s)
/Courier findfont
9 scalefont
setfont

newpath
11 86 moveto
({header}) show

showpage</code></pre>
<p>The bits that you may need to fiddle with, if you want to produce a different-sized barcode, are the initial location the ISSN symbol is drawn at (line 4) and <code>height=1.07</code> on the same line.</p>
<p>You’d also want to adjust the size specified to Ghostscript, which is used to trim the resulting image — the arguments are <code>-dDEVICEWIDTHPOINTS</code>, and <code>-dDEVICEHEIGHTPOINTS</code>.</p>
<p>I don’t know enough about PostScript (or Ghostscript) to give good general guidance about getting the right size. My advice would be to start with what I have and make small adjustments until you’re heading in the right direction (which is exactly how I settled on the arguments currently in use).</p>
<p>What I would emphasise is that if you have trouble with the existing Python modules that wrap BWIPP, it’s not difficult to use the PostScript directly yourself. Really, look back at the 16 lines of PostScript above — that’s it.</p>
<h3>Wrapping up</h3>
<p>By automating in this way, we now have a method where the person responsible for the back page simply clicks an icon in their dock, presses return when asked if they want the barcode for tomorrow, and everything else is taken care of.</p>
<p>Going back to our earlier discussion of hazards, I think we’ve reached the substitution stage rather than the elimination stage.</p>
<p>We have eliminated human error in choosing the components of the barcode, but we’ve done it by substituting code to make that decision. That’s still a good trade, because that code can be tested to ensure it does the right thing.</p>
<p>And then, you can go back to not worrying about barcodes.</p>]]></description>
            <pubDate>Wed, 28 Feb 2018 22:45:00 +0000</pubDate>
            <guid>https://robjwells.github.io/2018/02/british-newspaper-barcodes-explained-and-automated/</guid>
        </item>
        <item>
            <title><![CDATA[Colin Currie Group at Kings Place]]></title>
            <link>https://robjwells.github.io/2018/01/colin-currie-group-at-kings-place/</link>
            <description><![CDATA[<p>Last night, I treated myself to seeing the <a href="http://www.colincurriegroup.com">Colin Currie Group</a> <a href="https://www.kingsplace.co.uk/whats-on/contemporary/time-phase/">at Kings Place</a>, and they were absolutely sensational.</p>
<p>It was quite the programme, with six pieces of Steve Reich’s music, and closed with a performance of Quartet, composed by Reich for Currie’s group.</p>
<p>I first heard Quartet on May 24 2016, when it was played with Mallet Quartet and Music for 18 Musicians, and quite honestly was desperate to hear it again. It’s magical, and I left last night tapping out bits of it on my leg on the way to the train station.</p>
<p>It wasn’t online, apart for <a href="https://soundcloud.com/colin-currie-perc/colin-currie-group-steve-reich-quartet-excerpt">a short clip</a>, but last month <a href="https://www.youtube.com/watch?v=-8y8N4Z8tkQ">a video of the group performing Quartet</a> in full was posted to YouTube, and Nonesuch are releasing a recording of <a href="http://www.nonesuch.com/journal/steve-reich-pulse-quartet-february-2-nonesuch-records-2017-12-14">Pulse &amp; Quartet</a> (Pulse by <a href="https://www.iceorg.org">ICE</a>, Quartet by CCG) at the start of February.</p>
<p>The group are also releasing <a href="http://www.colincurriegroup.com/2018/01/pre-order-the-groups-first-recording/">a recording of Drumming</a>, which was as amazing to watch as it was to hear — I was sat right in line with the drums, and the effect of all four players drumming at once was stunning.</p>
<p>Last night was also the first time I’d heard New York Counterpoint (for clarinet) and Vermont Counterpoint (for flute) live, and they really opened my eyes, having sort-of ignored them before.</p>
<p>Really incredible stuff, all of it, start to finish. Reich’s compositions are brilliant, and I’m very thankful to be able to see a group dedicated to performing his work (for the third time now!).</p>]]></description>
            <pubDate>Sun, 21 Jan 2018 10:00:00 +0000</pubDate>
            <guid>https://robjwells.github.io/2018/01/colin-currie-group-at-kings-place/</guid>
        </item>
    </channel>
</rss>